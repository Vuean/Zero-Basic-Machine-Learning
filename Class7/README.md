# 第7课 循环神经网络——鉴定留言及探索系外行星

应用卷积网络处理图形图像效果很好。无论是普通的深度神经网络，还是卷积网络对样本特征的处理都是整体进行的，是**次序无关**的。在卷积网络中，虽然有一个通过滑动窗口抠取图块与卷积核进行卷积操作的过程，但对于每张图像来说仍然是一个整体操作。也就是说先处理左侧的特征图，还是先处理右侧的特征图，神经网络所得到的结果是完全相同的，预测值与操作特征的次序无关。

然而，在面对语言文字的时候特征之间的“次序”突然变得重要起来。本课中要讲的另一个重要神经网络模型——**循环神经网络**，就是专门用于处理语言、文字、时序这类特征之间存在“次序”的问题。这是一种循环的、带“记忆”功能的神经网络，这种网络针对序列性问题有其优势。

## 7.1 问题定义——鉴定评论文本的情感属性

文件中的Rating字段可视为评论文字属性的标签，即针对所购商品和本次采购行为的情感属性如下所示。

- Rating  5评价非常正面非常满意。

- Rating  4评价正面较为满意。

- Rating  3评价一般。

- Rating  2评价负面较不满意。

- Rating  1评价非常负面很不满意。

显而易见，如果机器习得了鉴别文字情感属性的能力，那么可以过滤垃圾留言和不文明的评论。有的时候针对某些网络留言可进行相关的预警工作，通过采取预防措施甚至能避免极端事件的发生。

## 7.2 循环神经网络的原理和结构

### 7.2.1 什么是序列数据

序列数据是其特征的先后顺序对于数据的解释和处理十分重要的数据。语音数据、文本数据都是序列数据。

文本数据集的形状为3D张量：**(样本,序号,字编码)**。

时间序列数据也有这种特点。这类数据是按时间顺序收集的，用于描述现象随时间变化的情况。如果不记录时戳，这些数字本身就没有意义。

时序数据集的形状为3D张量，(样本,时戳,标签)。

这些序列数据具体包括以下应用场景。

- 文档分类：比如识别新闻的主题或书的类型、作者。

- 文档或时间序列对比：比如估测两个文档或两支股票的相关程度。

- 文字情感分析比：如将评论、留言的情感划分为正面或负面。

- 时间序列预测：比如根据某地天气的历史数据来预测未来天气。

- 序列到序列的学习：比如两种语言之间的翻译。

### 7.2.2 前馈神经网络处理序列数据的局限性

普通人工神经网络和卷积神经网络可以称为**前馈神经网络（feedforward  neural  network）**，各神经元分层排列。每个神经元只与前一层的神经元相连，接收前一层的输出并输出给下一层，各层间没有反馈。每一层内部的神经元之间也没有任何反馈机制。

前馈神经网络也可以处理序列数据，但它是对数据整体读取、整体处理。比如一段文本需要整体输入神经网络，然后一次性地进行解释。每个单词处理过程中的权重是无差别的。网络并没有对相临近的两个单词进行特别的对待。

### 7.2.3 循环神经网络处理序列问题的策略

循环神经网络是一种具有**记忆功能**的神经网络，其特点是能够把刚刚处理过的信息放进神经网络的内存中。这样离目标近的特征单词的影响会比较大。

### 7.2.4 循环神经网络的结构

循环神经网络的结构与普通的前馈神经网络差异不大，其实最关键的地方有以下两处。

1. 以一段文字的处理为例，如果是普通的神经网络一段文字是整体读入网络处理——只处理一次，而循环神经网络则是每一个神经节点随着序列的发展处理N次，第一次处理一个字、第二次处理两个字直到处理完为止。

2. 循环神经网络的每个神经节点增加了一个对当前状态的记忆功能，也就是除了权重w和偏置b之外，循环神经网络的神经元中还多出一个当前状态的权重w。这个记录当前状态的w在网络学习的过程中就全权负责了对刚才所读的文字记忆的功能。

![fig01_普通网络的神经元](./figures/fig01_普通网络的神经元.jpg)

普通的神经网络中的神经元一次性读入全部特征作为其输入。

而循环神经网络的神经元需要沿着时间轴线，也就是向量X的“时戳”或“序号”特征维循环很多遍，因此也称RNN是带环的网络。这个“带环”指的是神经元，也就是网络节点自身带环，如下图所示。

![fig02_循环神经网络中的神经元](./figures/fig02_循环神经网络中的神经元.jpg)

多个循环神经网络的神经元在循环神经网络中组合的示意如下图所示。

![fig03_多个循环神经网络的神经元](./figures/fig03_多个循环神经网络的神经元.jpg)

如果把这个循环过程按序列进行展开，假设时间轴上有4个点，也就是4个序列特征，那么对于一个网络节点就要循环4次。这里引入隐状态h，并且需要多一个参数向量U用于实现网络的记忆功能。第一次读入特征时间点1时的状态如下图所示。

![fig04_时间点1读入一个特征](./figures/fig04_时间点1读入一个特征.jpg)

下一个时间点继续读入特征$x_2$，此时的状态已经变为$h_{t1}$，这个状态记忆着刚才读入$x_1$时的一些信息如下图所示。把这个状态与U进行点积运算。

![fig05_时间点2读入两个特征](./figures/fig05_时间点2读入两个特征.jpg)

持续进行时间轴上其他序列数据的处理，反复更新状态更新输出值。这里要强调的是，目前进行的只是一个神经元的操作，此处的W和U分别是一个向量。

![fig06_遍历特征处理](./figures/fig06_遍历特征处理.jpg)

时间轴上的节点遍历完成之后，循环就结束了。循环神经元向下一层网络输出$x'$。不难发现$x'$受最近的状态和最新的特征$x_4$的影响最大。

## 7.3 原始文本如何转换成向量数据

### 7.3.1 文本的向量化——分词

文本的向量化是机器学习进行进一步数据分析、理解、处理的基础。它的作用是令文本的内容尽可能地结构化。

不同类型的文本需要用到不同的处理方式。具体来说分为以下几种处理方式。

- 单字符的向量表达。

- 词语的向量表达。

- 短文本：如评论、留言等的向量表达。

- 长文本：如莎士比亚戏剧集的向量表达

最常见的情况是以“词语”为单位把文本进行向量化的表达。向量化这个过程也可以叫作**分词或切词（tokenization）**。

 ### 7.3.2 通过One-hot编码分词

 分词的最常见的方式是One-hot编码。One-hot编码很简单，就是是弄一个长长的单词表，也就是词典。每一个单词或字符、词组通过唯一整数索引i对应着词典里面的一个条目，然后将这个整数索引i转换为长度为N的二进制向量，N是词表大小。这个向量中只有第i个元素是1其余元素都为0。

下图给出了5部影片所形成的词典索引，当然就是15再转换为机器可
读的One-hot编码就是1000 0、01000等。
在Keras中使用Tokenizer类就可以轻松完成文本分词的功能